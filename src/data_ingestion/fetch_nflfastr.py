import os
import subprocess
import duckdb
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
DATABASE_URL = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
engine = create_engine(DATABASE_URL)

# --- Settings ---
seasons = [2019, 2020, 2021, 2022, 2023, 2024]
base_url = 'https://github.com/nflverse/nflverse-data/releases/download/player_stats/player_stats_{year}.parquet'
local_dir = "data/raw/nflfastr"
os.makedirs(local_dir, exist_ok=True)


# --- Download with curl to ensure redirects handled ---
def download_file_curl(url, output_path):
    if os.path.exists(output_path):
        print(f"‚úÖ Already exists: {output_path}")
        return
    print(f"‚¨áÔ∏è Downloading: {url}")
    subprocess.run(['curl', '-L', url, '-o', output_path], check=True)
    print(f"‚úÖ Saved: {output_path}")


# --- Download files for each season ---
files = []
for season in seasons:
    url = base_url.format(year=season)
    local_path = os.path.join(local_dir, f"player_stats_{season}.parquet")
    download_file_curl(url, local_path)
    files.append(local_path)


# --- Ingest with DuckDB ---
dfs = []
for file in files:
    if os.path.getsize(file) < 1000:
        raise ValueError(f"‚ùå File {file} is too small to be valid. Aborting.")
    print(f"üì• Processing {file} ...")
    query = f"""
        SELECT 
            season, 
            week, 
            player_name AS name, 
            passing_yards, 
            rushing_yards, 
            receiving_yards, 
            passing_tds + rushing_tds + receiving_tds AS touchdowns, 
            fantasy_points_ppr AS fantasy_points 
        FROM read_parquet('{file}')
    """
    df = duckdb.query(query).to_df()
    dfs.append(df)

combined_df = pd.concat(dfs)
print(f"‚úÖ Combined records: {len(combined_df)}")


# --- Map player_id from players table ---
players_df = pd.read_sql('SELECT player_id, name FROM players', engine)
merged_df = combined_df.merge(players_df, how='left', on='name')

# --- Report unmatched players ---
unmatched = merged_df[merged_df['player_id'].isnull()]
if not unmatched.empty:
    print("‚ö†Ô∏è Unmatched players (not in your `players` table):")
    print(unmatched['name'].dropna().unique())

# --- Filter out unmatched for DB insert ---
final_df = merged_df[merged_df['player_id'].notnull()]

# --- Prepare for Postgres insert ---
final_df = final_df[['player_id', 'season', 'week', 'passing_yards', 'rushing_yards', 'receiving_yards', 'touchdowns', 'fantasy_points']]

# --- Insert into Postgres ---
final_df.drop_duplicates().to_sql('weekly_stats', engine, if_exists='replace', index=False)
print(f"‚úÖ {len(final_df)} rows successfully ingested into `weekly_stats`.")
